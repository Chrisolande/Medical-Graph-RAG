{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "67e85"
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellUniqueIdByVincent": "6f537"
   },
   "outputs": [],
   "source": [
    "Entrez.email = \"olandechris@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "36e52"
   },
   "outputs": [],
   "source": [
    "def search_pubmed(query, max_results = 1, date_from = None, date_to = None, sort_order = \"relevance\", publication_types = None):\n",
    "    \n",
    "    search_term = query\n",
    "\n",
    "    # Add date filters\n",
    "    if date_from or date_to:\n",
    "        if date_from and date_to:\n",
    "            search_term += f' AND {date_from}[PDAT]:{date_to}[PDAT]'\n",
    "        elif date_from:\n",
    "            search_term += f' AND {date_from}[PDAT]:3000[PDAT]'\n",
    "        elif date_to:\n",
    "            search_term += f' AND 1900[PDAT]:{date_to}[PDAT]'\n",
    "\n",
    "    # Add publication type filters\n",
    "    if publication_types:\n",
    "        pub_filter = ' OR '.join([f'\"{pt}\"[Publication Type]' for pt in publication_types])\n",
    "        search_term += f' AND ({pub_filter})'\n",
    "    \n",
    "    print(f\"Searching PubMed with query: {search_term}\")\n",
    "\n",
    "    try: \n",
    "        # Perform the search\n",
    "        handle = Entrez.esearch(\n",
    "            db = \"pubmed\",\n",
    "            term=search_term,\n",
    "            retmax=max_results,\n",
    "            sort=sort_order\n",
    "        )\n",
    "\n",
    "        search_results = Entrez.read(handle)\n",
    "\n",
    "        handle.close()\n",
    "\n",
    "        pmids = search_results[\"IdList\"]\n",
    "        count = int(search_results[\"Count\"])\n",
    "        \n",
    "        print(f\"Found {count} total articles, retrieving {len(pmids)} IDs\")\n",
    "        return pmids\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return []\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellUniqueIdByVincent": "b4b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed with query: Kurt Cobain\n",
      "Found 4 total articles, retrieving 1 IDs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7988166']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_pubmed(\"Kurt Cobain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "029cb"
   },
   "outputs": [],
   "source": [
    "https://pubmed.ncbi.nlm.nih.gov/7988166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellUniqueIdByVincent": "42b51"
   },
   "outputs": [],
   "source": [
    "def parse_article(summary, record):\n",
    "    \"\"\"Parse article summary and record into structured data\"\"\"\n",
    "    try:\n",
    "        # Basic info from summary\n",
    "        pmid = str(summary.get('Id', ''))\n",
    "        title = summary.get('Title', '').strip()\n",
    "        \n",
    "        # Journal info\n",
    "        journal = summary.get('Source', '')\n",
    "        pub_date = summary.get('PubDate', '')\n",
    "        \n",
    "        # Authors from summary\n",
    "        authors_list = summary.get('AuthorList', [])\n",
    "        authors = '; '.join([author for author in authors_list]) if authors_list else ''\n",
    "        \n",
    "        # Extract more details from full record\n",
    "        article = record['MedlineCitation']['Article']\n",
    "        \n",
    "        # Abstract\n",
    "        abstract = ''\n",
    "        if 'Abstract' in article:\n",
    "            abstract_texts = []\n",
    "            if 'AbstractText' in article['Abstract']:\n",
    "                for abs_text in article['Abstract']['AbstractText']:\n",
    "                    if isinstance(abs_text, str):\n",
    "                        abstract_texts.append(abs_text)\n",
    "                    else:\n",
    "                        # Handle structured abstracts with labels\n",
    "                        abstract_texts.append(str(abs_text))\n",
    "            abstract = ' '.join(abstract_texts)\n",
    "        \n",
    "        # Publication details\n",
    "        journal_info = article.get('Journal', {})\n",
    "        journal_title = journal_info.get('Title', journal)\n",
    "        \n",
    "        # Volume and issue\n",
    "        journal_issue = journal_info.get('JournalIssue', {})\n",
    "        volume = journal_issue.get('Volume', '')\n",
    "        issue = journal_issue.get('Issue', '')\n",
    "        \n",
    "        # Publication date details\n",
    "        pub_date_info = journal_issue.get('PubDate', {})\n",
    "        year = pub_date_info.get('Year', '')\n",
    "        month = pub_date_info.get('Month', '')\n",
    "        day = pub_date_info.get('Day', '')\n",
    "        \n",
    "        # DOI and other identifiers\n",
    "        doi = ''\n",
    "        pmc_id = ''\n",
    "        \n",
    "        if 'ELocationID' in article:\n",
    "            for eloc in article['ELocationID']:\n",
    "                if eloc.attributes.get('EIdType') == 'doi':\n",
    "                    doi = str(eloc)\n",
    "                elif eloc.attributes.get('EIdType') == 'pmc':\n",
    "                    pmc_id = str(eloc)\n",
    "        \n",
    "        # Keywords/MeSH terms\n",
    "        mesh_terms = []\n",
    "        if 'MeshHeadingList' in record['MedlineCitation']:\n",
    "            for mesh in record['MedlineCitation']['MeshHeadingList']:\n",
    "                descriptor = mesh['DescriptorName']\n",
    "                mesh_terms.append(str(descriptor))\n",
    "        \n",
    "        # Publication types\n",
    "        pub_types = []\n",
    "        if 'PublicationTypeList' in article:\n",
    "            pub_types = [str(pt) for pt in article['PublicationTypeList']]\n",
    "        \n",
    "        return {\n",
    "            'pmid': pmid,\n",
    "            'title': title,\n",
    "            'abstract': abstract,\n",
    "            'authors': authors,\n",
    "            'journal': journal_title,\n",
    "            'volume': volume,\n",
    "            'issue': issue,\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': day,\n",
    "            'pub_date': pub_date,\n",
    "            'doi': doi,\n",
    "            'pmc_id': pmc_id,\n",
    "            'mesh_terms': '; '.join(mesh_terms),\n",
    "            'publication_types': '; '.join(pub_types),\n",
    "            'pubmed_url': f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\",\n",
    "            'doi_url': f\"https://doi.org/{doi}\" if doi else ''\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing article {pmid}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellUniqueIdByVincent": "bcdb1"
   },
   "outputs": [],
   "source": [
    "def fetch_article_details(pmids, batch_size = 100):\n",
    "    articles = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch_pmids = pmids[i:i + batch_size]\n",
    "\n",
    "        print(f\"Fetching batch {i//batch_size + 1}/{(len(pmids)-1)//batch_size + 1} \"\n",
    "                  f\"({len(batch_pmids)} articles)...\")\n",
    "\n",
    "        try:\n",
    "            # Fetch article summaries first\n",
    "            handle = Entrez.esummary(db = \"pubmed\", id = \",\".join(batch_pmids))\n",
    "            summaries = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            # Fetch full abstract records\n",
    "            handle = Entrez.efetch(\n",
    "                db = \"pubmed\",\n",
    "                id = \",\".join(batch_pmids),\n",
    "                rettype = \"medline\",\n",
    "                retmode = \"xml\"\n",
    "            )\n",
    "\n",
    "            records = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            # Parse articles\n",
    "            for summary, record in zip(summaries, records['PubmedArticle']):\n",
    "                article_data = parse_article(summary, record)\n",
    "                if article_data:\n",
    "                    articles.append(article_data)\n",
    "            \n",
    "            # Simple rate limiter\n",
    "            #TODO: Implement the exponential backoff if need be\n",
    "            time.sleep(0.34) # 3 requests per second\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch {i//batch_size + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellUniqueIdByVincent": "e268e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching batch 1/1 (1 articles)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pmid': '7988166',\n",
       "  'title': 'Kurt Cobain.',\n",
       "  'abstract': '',\n",
       "  'authors': 'Kienhorst I',\n",
       "  'journal': 'Crisis',\n",
       "  'volume': '15',\n",
       "  'issue': '2',\n",
       "  'year': '1994',\n",
       "  'month': '',\n",
       "  'day': '',\n",
       "  'pub_date': '1994',\n",
       "  'doi': '',\n",
       "  'pmc_id': '',\n",
       "  'mesh_terms': 'Adolescent; Adult; Europe; Famous Persons; Female; History, 20th Century; Humans; Imitative Behavior; Male; Music; Social Conformity; Suicide; United States; Wounds, Gunshot',\n",
       "  'publication_types': 'Biography; Historical Article; Journal Article',\n",
       "  'pubmed_url': 'https://pubmed.ncbi.nlm.nih.gov/7988166/',\n",
       "  'doi_url': ''}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_article_details(['7988166'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellUniqueIdByVincent": "57bff"
   },
   "outputs": [],
   "source": [
    "def advanced_search(**kwargs):\n",
    "    \"\"\"\n",
    "    Perform advanced search with multiple parameters\n",
    "    \n",
    "    Available parameters:\n",
    "    - query: Main search terms\n",
    "    - author: Author name\n",
    "    - journal: Journal name\n",
    "    - mesh_terms: MeSH terms list\n",
    "    - title_words: Words that must appear in title\n",
    "    - abstract_words: Words that must appear in abstract\n",
    "    - date_from/date_to: Date range\n",
    "    - publication_types: List of publication types\n",
    "    - languages: List of languages\n",
    "    - max_results: Maximum results\n",
    "    \"\"\"\n",
    "\n",
    "    search_parts = []\n",
    "    # Main query\n",
    "    if \"query\" in kwargs:\n",
    "        search_parts.append(kwargs[\"query\"])\n",
    "\n",
    "    # Author\n",
    "    if \"author\" in kwargs:\n",
    "        search_parts.append(f'\"{kwargs[\"author\"]}\"[Author]')\n",
    "    \n",
    "    # Journal\n",
    "    if \"journal\" in kwargs:\n",
    "        search_parts.append(f'\"{kwargs[\"journal\"]}\"[Journal]')\n",
    "\n",
    "    # Mesh terms\n",
    "    if \"mesh_terms\" in kwargs:\n",
    "        mesh_queries = [f'\"{term}\"[MeSH Terms]' for term in kwargs['mesh_terms']]\n",
    "        search_parts.append(f'({\" OR \".join(mesh_queries)})')\n",
    "\n",
    "    # Title words\n",
    "    if 'title_words' in kwargs:\n",
    "        title_queries = [f'\"{word}\"[Title]' for word in kwargs['title_words']]\n",
    "        search_parts.append(f'({\" AND \".join(title_queries)})')\n",
    "    \n",
    "    # Abstract words\n",
    "    if 'abstract_words' in kwargs:\n",
    "        abstract_queries = [f'\"{word}\"[Abstract]' for word in kwargs['abstract_words']]\n",
    "        search_parts.append(f'({\" AND \".join(abstract_queries)})')\n",
    "    \n",
    "    # Languages\n",
    "    if 'languages' in kwargs:\n",
    "        lang_queries = [f'\"{lang}\"[Language]' for lang in kwargs['languages']]\n",
    "        search_parts.append(f'({\" OR \".join(lang_queries)})')\n",
    "    \n",
    "    # Combine all parts\n",
    "    full_query = ' AND '.join(search_parts)\n",
    "\n",
    "    return search_pubmed(\n",
    "            query=full_query,\n",
    "            max_results=kwargs.get('max_results', 4),\n",
    "            date_from=kwargs.get('date_from'),\n",
    "            date_to=kwargs.get('date_to'),\n",
    "            publication_types=kwargs.get('publication_types')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellUniqueIdByVincent": "0df27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed with query: Kurt Cobain\n",
      "Found 4 total articles, retrieving 4 IDs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7988166', '8897665', '26445123', '16179336']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advanced_search(query = \"Kurt Cobain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellUniqueIdByVincent": "07fd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching batch 1/1 (4 articles)...\n"
     ]
    }
   ],
   "source": [
    "articles = fetch_article_details(['7988166', '8897665', '26445123', '16179336'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellUniqueIdByVincent": "df681"
   },
   "outputs": [],
   "source": [
    "def get_article_statistics(articles):\n",
    "        \"\"\"Generate basic statistics about downloaded articles\"\"\"\n",
    "        if not articles:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(articles)\n",
    "        \n",
    "        stats = {\n",
    "            'total_articles': len(articles),\n",
    "            'articles_with_abstracts': len(df[df['abstract'].str.len() > 0]),\n",
    "            'date_range': {\n",
    "                'earliest': df['year'].min(),\n",
    "                'latest': df['year'].max()\n",
    "            },\n",
    "            'top_journals': df['journal'].value_counts().head(10).to_dict(),\n",
    "            'publication_types': df['publication_types'].value_counts().head(10).to_dict(),\n",
    "            'articles_per_year': df['year'].value_counts().sort_index().to_dict()\n",
    "        }\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellUniqueIdByVincent": "bdf9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_articles': 4,\n",
       " 'articles_with_abstracts': 3,\n",
       " 'date_range': {'earliest': '1994', 'latest': '2015'},\n",
       " 'top_journals': {'Crisis': 1,\n",
       "  'Suicide & life-threatening behavior': 1,\n",
       "  'Arquivos de neuro-psiquiatria': 1,\n",
       "  'Archives of suicide research : official journal of the International Academy for Suicide Research': 1},\n",
       " 'publication_types': {'Biography; Historical Article; Journal Article': 2,\n",
       "  'Historical Article; Journal Article; Portrait': 1,\n",
       "  'Case Reports; Journal Article': 1},\n",
       " 'articles_per_year': {'1994': 1, '1996': 1, '2005': 1, '2015': 1}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_article_statistics(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellUniqueIdByVincent": "957d1"
   },
   "outputs": [],
   "source": [
    "def save_to_csv(articles, filename):\n",
    "    \"\"\"Save articles to CSV using pandas\"\"\"\n",
    "    if not articles:\n",
    "        print(\"No articles to save\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Saved {len(articles)} articles to {filename}\")\n",
    "\n",
    "def save_to_excel(articles, filename):\n",
    "    \"\"\"Save articles to Excel file\"\"\"\n",
    "    if not articles:\n",
    "        print(\"No articles to save\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_excel(filename, index=False, engine='openpyxl')\n",
    "    print(f\"Saved {len(articles)} articles to {filename}\")\n",
    "\n",
    "def save_to_json(articles, filename):\n",
    "    \"\"\"Save articles to JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved {len(articles)} articles to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellUniqueIdByVincent": "51ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 articles to Cobain.csv\n"
     ]
    }
   ],
   "source": [
    "save_to_csv(articles, \"Cobain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellUniqueIdByVincent": "80e68"
   },
   "source": [
    "# The mismatch count returned by the vector store\n",
    "The vector store says that the number of documents in the graph is 2451, while we uploaded 4833 documents, a suggestion that we might be duplicating things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellUniqueIdByVincent": "d4834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents (chunks) loaded from JSON: 4833\n",
      "Length of the 'documents' list after list comprehension: 4833\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents_path = \"output/pmc_chunks/pmc_semantic_chunks.json\"\n",
    "\n",
    "with open(documents_path, \"r\", encoding = \"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "loaded_documents_count = len(data.get(\"documents\", []))\n",
    "print(f\"Number of documents (chunks) loaded from JSON: {loaded_documents_count}\")\n",
    "\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in data.get(\"documents\", [])]\n",
    "print(f\"Length of the 'documents' list after list comprehension: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellUniqueIdByVincent": "c9e33"
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Group by pmid to see the pmid_seq_num pattern\n",
    "pmid_groups = defaultdict(list)\n",
    "doc_ids = []\n",
    "\n",
    "for doc in documents:\n",
    "    pmid = doc.metadata.get(\"pmid\", \" \")\n",
    "    seq_num = doc.metadata.get(\"seq_num\", \" \")\n",
    "\n",
    "    doc_id = f\"{pmid}_{seq_num}\"\n",
    "\n",
    "    pmid_groups[pmid].append(seq_num)\n",
    "    doc_ids.append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellUniqueIdByVincent": "7aabe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 4833\n",
      "Unique PMIDs: 2451\n",
      "Duplicate pmid_seq_num combinations: 2366\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "id_counts = Counter(doc_ids)\n",
    "duplicates = {doc_id: count for doc_id, count in id_counts.items() if count > 1}\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Unique PMIDs: {len(pmid_groups)}\")\n",
    "print(f\"Duplicate pmid_seq_num combinations: {len(duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "24f22"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "205dfd640fc5b077051ac09e_2025-06-04T10-49-23-812Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
