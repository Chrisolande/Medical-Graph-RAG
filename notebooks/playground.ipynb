{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "cellUniqueIdByVincent": "67e85"
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "cellUniqueIdByVincent": "6f537"
   },
   "outputs": [],
   "source": [
    "Entrez.email = \"olandechris@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "cellUniqueIdByVincent": "36e52"
   },
   "outputs": [],
   "source": [
    "def search_pubmed(query, max_results = 1, date_from = None, date_to = None, sort_order = \"relevance\", publication_types = None):\n",
    "    \n",
    "    search_term = query\n",
    "\n",
    "    # Add date filters\n",
    "    if date_from or date_to:\n",
    "        if date_from and date_to:\n",
    "            search_term += f' AND {date_from}[PDAT]:{date_to}[PDAT]'\n",
    "        elif date_from:\n",
    "            search_term += f' AND {date_from}[PDAT]:3000[PDAT]'\n",
    "        elif date_to:\n",
    "            search_term += f' AND 1900[PDAT]:{date_to}[PDAT]'\n",
    "\n",
    "    # Add publication type filters\n",
    "    if publication_types:\n",
    "        pub_filter = ' OR '.join([f'\"{pt}\"[Publication Type]' for pt in publication_types])\n",
    "        search_term += f' AND ({pub_filter})'\n",
    "    \n",
    "    print(f\"Searching PubMed with query: {search_term}\")\n",
    "\n",
    "    try: \n",
    "        # Perform the search\n",
    "        handle = Entrez.esearch(\n",
    "            db = \"pubmed\",\n",
    "            term=search_term,\n",
    "            retmax=max_results,\n",
    "            sort=sort_order\n",
    "        )\n",
    "\n",
    "        search_results = Entrez.read(handle)\n",
    "\n",
    "        handle.close()\n",
    "\n",
    "        pmids = search_results[\"IdList\"]\n",
    "        count = int(search_results[\"Count\"])\n",
    "        \n",
    "        print(f\"Found {count} total articles, retrieving {len(pmids)} IDs\")\n",
    "        return pmids\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "        return []\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "cellUniqueIdByVincent": "b4b05"
   },
   "outputs": [],
   "source": [
    "search_pubmed(\"Kurt Cobain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "cellUniqueIdByVincent": "029cb"
   },
   "outputs": [],
   "source": [
    "https://pubmed.ncbi.nlm.nih.gov/7988166"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "cellUniqueIdByVincent": "42b51"
   },
   "outputs": [],
   "source": [
    "def parse_article(summary, record):\n",
    "    \"\"\"Parse article summary and record into structured data\"\"\"\n",
    "    try:\n",
    "        # Basic info from summary\n",
    "        pmid = str(summary.get('Id', ''))\n",
    "        title = summary.get('Title', '').strip()\n",
    "        \n",
    "        # Journal info\n",
    "        journal = summary.get('Source', '')\n",
    "        pub_date = summary.get('PubDate', '')\n",
    "        \n",
    "        # Authors from summary\n",
    "        authors_list = summary.get('AuthorList', [])\n",
    "        authors = '; '.join([author for author in authors_list]) if authors_list else ''\n",
    "        \n",
    "        # Extract more details from full record\n",
    "        article = record['MedlineCitation']['Article']\n",
    "        \n",
    "        # Abstract\n",
    "        abstract = ''\n",
    "        if 'Abstract' in article:\n",
    "            abstract_texts = []\n",
    "            if 'AbstractText' in article['Abstract']:\n",
    "                for abs_text in article['Abstract']['AbstractText']:\n",
    "                    if isinstance(abs_text, str):\n",
    "                        abstract_texts.append(abs_text)\n",
    "                    else:\n",
    "                        # Handle structured abstracts with labels\n",
    "                        abstract_texts.append(str(abs_text))\n",
    "            abstract = ' '.join(abstract_texts)\n",
    "        \n",
    "        # Publication details\n",
    "        journal_info = article.get('Journal', {})\n",
    "        journal_title = journal_info.get('Title', journal)\n",
    "        \n",
    "        # Volume and issue\n",
    "        journal_issue = journal_info.get('JournalIssue', {})\n",
    "        volume = journal_issue.get('Volume', '')\n",
    "        issue = journal_issue.get('Issue', '')\n",
    "        \n",
    "        # Publication date details\n",
    "        pub_date_info = journal_issue.get('PubDate', {})\n",
    "        year = pub_date_info.get('Year', '')\n",
    "        month = pub_date_info.get('Month', '')\n",
    "        day = pub_date_info.get('Day', '')\n",
    "        \n",
    "        # DOI and other identifiers\n",
    "        doi = ''\n",
    "        pmc_id = ''\n",
    "        \n",
    "        if 'ELocationID' in article:\n",
    "            for eloc in article['ELocationID']:\n",
    "                if eloc.attributes.get('EIdType') == 'doi':\n",
    "                    doi = str(eloc)\n",
    "                elif eloc.attributes.get('EIdType') == 'pmc':\n",
    "                    pmc_id = str(eloc)\n",
    "        \n",
    "        # Keywords/MeSH terms\n",
    "        mesh_terms = []\n",
    "        if 'MeshHeadingList' in record['MedlineCitation']:\n",
    "            for mesh in record['MedlineCitation']['MeshHeadingList']:\n",
    "                descriptor = mesh['DescriptorName']\n",
    "                mesh_terms.append(str(descriptor))\n",
    "        \n",
    "        # Publication types\n",
    "        pub_types = []\n",
    "        if 'PublicationTypeList' in article:\n",
    "            pub_types = [str(pt) for pt in article['PublicationTypeList']]\n",
    "        \n",
    "        return {\n",
    "            'pmid': pmid,\n",
    "            'title': title,\n",
    "            'abstract': abstract,\n",
    "            'authors': authors,\n",
    "            'journal': journal_title,\n",
    "            'volume': volume,\n",
    "            'issue': issue,\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'day': day,\n",
    "            'pub_date': pub_date,\n",
    "            'doi': doi,\n",
    "            'pmc_id': pmc_id,\n",
    "            'mesh_terms': '; '.join(mesh_terms),\n",
    "            'publication_types': '; '.join(pub_types),\n",
    "            'pubmed_url': f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\",\n",
    "            'doi_url': f\"https://doi.org/{doi}\" if doi else ''\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing article {pmid}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "cellUniqueIdByVincent": "bcdb1"
   },
   "outputs": [],
   "source": [
    "def fetch_article_details(pmids, batch_size = 100):\n",
    "    articles = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(pmids), batch_size):\n",
    "        batch_pmids = pmids[i:i + batch_size]\n",
    "\n",
    "        print(f\"Fetching batch {i//batch_size + 1}/{(len(pmids)-1)//batch_size + 1} \"\n",
    "                  f\"({len(batch_pmids)} articles)...\")\n",
    "\n",
    "        try:\n",
    "            # Fetch article summaries first\n",
    "            handle = Entrez.esummary(db = \"pubmed\", id = \",\".join(batch_pmids))\n",
    "            summaries = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            # Fetch full abstract records\n",
    "            handle = Entrez.efetch(\n",
    "                db = \"pubmed\",\n",
    "                id = \",\".join(batch_pmids),\n",
    "                rettype = \"medline\",\n",
    "                retmode = \"xml\"\n",
    "            )\n",
    "\n",
    "            records = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            # Parse articles\n",
    "            for summary, record in zip(summaries, records['PubmedArticle']):\n",
    "                article_data = parse_article(summary, record)\n",
    "                if article_data:\n",
    "                    articles.append(article_data)\n",
    "            \n",
    "            # Simple rate limiter\n",
    "            #TODO: Implement the exponential backoff if need be\n",
    "            time.sleep(0.34) # 3 requests per second\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch {i//batch_size + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "cellUniqueIdByVincent": "e268e"
   },
   "outputs": [],
   "source": [
    "fetch_article_details(['7988166'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "cellUniqueIdByVincent": "57bff"
   },
   "outputs": [],
   "source": [
    "def advanced_search(**kwargs):\n",
    "    \"\"\"\n",
    "    Perform advanced search with multiple parameters\n",
    "    \n",
    "    Available parameters:\n",
    "    - query: Main search terms\n",
    "    - author: Author name\n",
    "    - journal: Journal name\n",
    "    - mesh_terms: MeSH terms list\n",
    "    - title_words: Words that must appear in title\n",
    "    - abstract_words: Words that must appear in abstract\n",
    "    - date_from/date_to: Date range\n",
    "    - publication_types: List of publication types\n",
    "    - languages: List of languages\n",
    "    - max_results: Maximum results\n",
    "    \"\"\"\n",
    "\n",
    "    search_parts = []\n",
    "    # Main query\n",
    "    if \"query\" in kwargs:\n",
    "        search_parts.append(kwargs[\"query\"])\n",
    "\n",
    "    # Author\n",
    "    if \"author\" in kwargs:\n",
    "        search_parts.append(f'\"{kwargs[\"author\"]}\"[Author]')\n",
    "    \n",
    "    # Journal\n",
    "    if \"journal\" in kwargs:\n",
    "        search_parts.append(f'\"{kwargs[\"journal\"]}\"[Journal]')\n",
    "\n",
    "    # Mesh terms\n",
    "    if \"mesh_terms\" in kwargs:\n",
    "        mesh_queries = [f'\"{term}\"[MeSH Terms]' for term in kwargs['mesh_terms']]\n",
    "        search_parts.append(f'({\" OR \".join(mesh_queries)})')\n",
    "\n",
    "    # Title words\n",
    "    if 'title_words' in kwargs:\n",
    "        title_queries = [f'\"{word}\"[Title]' for word in kwargs['title_words']]\n",
    "        search_parts.append(f'({\" AND \".join(title_queries)})')\n",
    "    \n",
    "    # Abstract words\n",
    "    if 'abstract_words' in kwargs:\n",
    "        abstract_queries = [f'\"{word}\"[Abstract]' for word in kwargs['abstract_words']]\n",
    "        search_parts.append(f'({\" AND \".join(abstract_queries)})')\n",
    "    \n",
    "    # Languages\n",
    "    if 'languages' in kwargs:\n",
    "        lang_queries = [f'\"{lang}\"[Language]' for lang in kwargs['languages']]\n",
    "        search_parts.append(f'({\" OR \".join(lang_queries)})')\n",
    "    \n",
    "    # Combine all parts\n",
    "    full_query = ' AND '.join(search_parts)\n",
    "\n",
    "    return search_pubmed(\n",
    "            query=full_query,\n",
    "            max_results=kwargs.get('max_results', 4),\n",
    "            date_from=kwargs.get('date_from'),\n",
    "            date_to=kwargs.get('date_to'),\n",
    "            publication_types=kwargs.get('publication_types')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "cellUniqueIdByVincent": "0df27"
   },
   "outputs": [],
   "source": [
    "advanced_search(query = \"Kurt Cobain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "cellUniqueIdByVincent": "07fd0"
   },
   "outputs": [],
   "source": [
    "articles = fetch_article_details(['7988166', '8897665', '26445123', '16179336'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "cellUniqueIdByVincent": "df681"
   },
   "outputs": [],
   "source": [
    "def get_article_statistics(articles):\n",
    "        \"\"\"Generate basic statistics about downloaded articles\"\"\"\n",
    "        if not articles:\n",
    "            return {}\n",
    "        \n",
    "        df = pd.DataFrame(articles)\n",
    "        \n",
    "        stats = {\n",
    "            'total_articles': len(articles),\n",
    "            'articles_with_abstracts': len(df[df['abstract'].str.len() > 0]),\n",
    "            'date_range': {\n",
    "                'earliest': df['year'].min(),\n",
    "                'latest': df['year'].max()\n",
    "            },\n",
    "            'top_journals': df['journal'].value_counts().head(10).to_dict(),\n",
    "            'publication_types': df['publication_types'].value_counts().head(10).to_dict(),\n",
    "            'articles_per_year': df['year'].value_counts().sort_index().to_dict()\n",
    "        }\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "cellUniqueIdByVincent": "bdf9b"
   },
   "outputs": [],
   "source": [
    "get_article_statistics(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "cellUniqueIdByVincent": "957d1"
   },
   "outputs": [],
   "source": [
    "def save_to_csv(articles, filename):\n",
    "    \"\"\"Save articles to CSV using pandas\"\"\"\n",
    "    if not articles:\n",
    "        print(\"No articles to save\")\n",
    "        return\n",
    "        \n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Saved {len(articles)} articles to {filename}\")\n",
    "\n",
    "def save_to_excel(articles, filename):\n",
    "    \"\"\"Save articles to Excel file\"\"\"\n",
    "    if not articles:\n",
    "        print(\"No articles to save\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_excel(filename, index=False, engine='openpyxl')\n",
    "    print(f\"Saved {len(articles)} articles to {filename}\")\n",
    "\n",
    "def save_to_json(articles, filename):\n",
    "    \"\"\"Save articles to JSON file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved {len(articles)} articles to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "cellUniqueIdByVincent": "51ad9"
   },
   "outputs": [],
   "source": [
    "save_to_csv(articles, \"Cobain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "cellUniqueIdByVincent": "80e68"
   },
   "source": [
    "# The mismatch count returned by the vector store\n",
    "The vector store says that the number of documents in the graph is 2451, while we uploaded 4833 documents, a suggestion that we might be duplicating things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "cellUniqueIdByVincent": "d4834"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents_path = \"output/pmc_chunks/pmc_semantic_chunks.json\"\n",
    "\n",
    "with open(documents_path, \"r\", encoding = \"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "loaded_documents_count = len(data.get(\"documents\", []))\n",
    "print(f\"Number of documents (chunks) loaded from JSON: {loaded_documents_count}\")\n",
    "\n",
    "documents = [Document(page_content=doc[\"content\"], metadata=doc[\"metadata\"]) for doc in data.get(\"documents\", [])]\n",
    "print(f\"Length of the 'documents' list after list comprehension: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "cellUniqueIdByVincent": "c9e33"
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Group by pmid to see the pmid_seq_num pattern\n",
    "pmid_groups = defaultdict(list)\n",
    "doc_ids = []\n",
    "\n",
    "for doc in documents:\n",
    "    pmid = doc.metadata.get(\"pmid\", \" \")\n",
    "    seq_num = doc.metadata.get(\"seq_num\", \" \")\n",
    "\n",
    "    doc_id = f\"{pmid}_{seq_num}\"\n",
    "\n",
    "    pmid_groups[pmid].append(seq_num)\n",
    "    doc_ids.append(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "cellUniqueIdByVincent": "7aabe"
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "id_counts = Counter(doc_ids)\n",
    "duplicates = {doc_id: count for doc_id, count in id_counts.items() if count > 1}\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"Unique PMIDs: {len(pmid_groups)}\")\n",
    "print(f\"Duplicate pmid_seq_num combinations: {len(duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "cellUniqueIdByVincent": "bebeb"
   },
   "outputs": [],
   "source": [
    "pmid_groups = defaultdict(list)\n",
    "for i, doc in enumerate(documents):\n",
    "    pmid = doc.metadata.get(\"pmid\", \" \")\n",
    "    pmid_groups[pmid].append((i, doc))\n",
    "\n",
    "fixed_count = 0\n",
    "for pmid, doc_list in pmid_groups.items():\n",
    "    if len(doc_list) > 1: # Process pmids with multiple chunks\n",
    "        print(f\"Fixing PMID {pmid}: {len(doc_list)} chunks\")\n",
    "\n",
    "        # Sort by original order\n",
    "        doc_list.sort(key = lambda x: x[0])\n",
    "\n",
    "        # Reassign seq_num: 0, 1, 2, 3...\n",
    "        for new_seq, (original_idx, doc) in enumerate(doc_list):\n",
    "            old_seq = doc.metadata.get('seq_num', '')\n",
    "            doc.metadata['seq_num'] = new_seq\n",
    "            print(f\"  Changed seq_num from {old_seq} to {new_seq}\")\n",
    "            fixed_count += 1\n",
    "\n",
    "print(f\"Fixed {fixed_count} documents total\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "cellUniqueIdByVincent": "36e48"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Group documents by pmid\n",
    "pmid_groups = defaultdict(list)\n",
    "for i, doc in enumerate(documents):\n",
    "    pmid = doc.metadata.get('pmid', '')\n",
    "    pmid_groups[pmid].append((i, doc))\n",
    "\n",
    "# Fix seq_num for each pmid group\n",
    "fixed_count = 0\n",
    "for pmid, doc_list in pmid_groups.items():\n",
    "    if len(doc_list) > 1:  # Only process PMIDs with multiple chunks\n",
    "        print(f\"Fixing PMID {pmid}: {len(doc_list)} chunks\")\n",
    "        \n",
    "        # Sort by original order\n",
    "        doc_list.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Reassign seq_num: 0, 1, 2, 3...\n",
    "        for new_seq, (original_idx, doc) in enumerate(doc_list):\n",
    "            old_seq = doc.metadata.get('seq_num', '')\n",
    "            doc.metadata['seq_num'] = new_seq\n",
    "            print(f\"  Changed seq_num from {old_seq} to {new_seq}\")\n",
    "            fixed_count += 1\n",
    "\n",
    "print(f\"Fixed {fixed_count} documents total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "semantic_cache = SemanticCache(\n",
    "    database_path=\".my_semantic_cache.db\",\n",
    "    faiss_index_path=\"./my_semantic_faiss_index\",\n",
    "    similarity_threshold=0.5,\n",
    "    max_cache_size=1000,        # FAISS index size limit\n",
    "    memory_cache_size=100,      # In-memory cache size\n",
    "    batch_size=10,              # Batch processing size\n",
    "    enable_quantization=True    # Enable for large datasets\n",
    ")\n",
    "\n",
    "set_llm_cache(semantic_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_cache.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name= \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "documents = [\n",
    "        \"Python is a high-level programming language known for its simplicity\",\n",
    "        \"Java is an object-oriented programming language used for enterprise applications\", \n",
    "        \"Machine learning algorithms automatically learn patterns from data\",\n",
    "        \"Deep learning is a subset of machine learning using neural networks\",\n",
    "        \"FAISS enables efficient similarity search and clustering of dense vectors\",\n",
    "        \"Elasticsearch is a distributed search and analytics engine\",\n",
    "        \"BM25 is a probabilistic ranking function used in information retrieval\",\n",
    "        \"TF-IDF measures term importance in document collections\",\n",
    "        \"Vector databases are optimized for storing high-dimensional embeddings\",\n",
    "        \"SQL databases use structured query language for data management\",\n",
    "        \"Natural language processing enables computers to understand human language\",\n",
    "        \"Computer vision algorithms process and analyze visual data\",\n",
    "        \"Recommendation systems suggest relevant items to users\",\n",
    "        \"Time series analysis forecasts future values from historical data\",\n",
    "        \"Graph databases model relationships between entities\"\n",
    "    ]\n",
    "for doc in documents:\n",
    "    print(embedding.embed_query(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "class RetrievalEvaluator:\n",
    "    def __init__(self):\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name= \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "    async def setup_retrievers(self, documents: List[str]) -> Dict:\n",
    "        \"\"\"Setup FAISS, BM25, and RRF retrievers\"\"\"\n",
    "        docs = [Document(page_content=doc) for doc in documents]\n",
    "        \n",
    "        # FAISS retriever\n",
    "        faiss_store = await FAISS.afrom_documents(docs, self.embeddings)\n",
    "        faiss_retriever = faiss_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "        \n",
    "        # BM25 retriever\n",
    "        bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "        bm25_retriever.k = 5\n",
    "        \n",
    "        # RRF (Ensemble) retriever combining FAISS + BM25\n",
    "        rrf_retriever = EnsembleRetriever(\n",
    "            retrievers=[faiss_retriever, bm25_retriever],\n",
    "            weights=[0.5, 0.5]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"faiss\": faiss_retriever,\n",
    "            \"bm25\": bm25_retriever, \n",
    "            \"rrf\": rrf_retriever\n",
    "        }\n",
    "    \n",
    "    async def evaluate_retrieval(self, retrievers: Dict, queries: List[str], \n",
    "                               ground_truth: List[List[int]]) -> Dict:\n",
    "        \"\"\"Evaluate retrievers using MRR, Recall@K, and Precision@K\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, retriever in retrievers.items():\n",
    "            mrr_scores = []\n",
    "            recall_scores = []\n",
    "            precision_scores = []\n",
    "            \n",
    "            for query, relevant_docs in zip(queries, ground_truth):\n",
    "                retrieved = await retriever.ainvoke(query)\n",
    "                retrieved_indices = [int(doc.metadata.get('index', -1)) \n",
    "                                   for doc in retrieved if 'index' in doc.metadata]\n",
    "                \n",
    "                # MRR calculation\n",
    "                mrr = 0\n",
    "                for i, doc_idx in enumerate(retrieved_indices):\n",
    "                    if doc_idx in relevant_docs:\n",
    "                        mrr = 1 / (i + 1)\n",
    "                        break\n",
    "                mrr_scores.append(mrr)\n",
    "                \n",
    "                # Recall@K and Precision@K\n",
    "                relevant_retrieved = set(retrieved_indices) & set(relevant_docs)\n",
    "                recall = len(relevant_retrieved) / len(relevant_docs) if relevant_docs else 0\n",
    "                precision = len(relevant_retrieved) / len(retrieved_indices) if retrieved_indices else 0\n",
    "                \n",
    "                recall_scores.append(recall)\n",
    "                precision_scores.append(precision)\n",
    "            \n",
    "            results[name] = {\n",
    "                \"MRR\": np.mean(mrr_scores),\n",
    "                \"Recall@5\": np.mean(recall_scores),\n",
    "                \"Precision@5\": np.mean(precision_scores)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Usage example\n",
    "async def main():\n",
    "    # More realistic dataset with noise\n",
    "    documents = [\n",
    "        \"Python is a high-level programming language known for its simplicity\",\n",
    "        \"Java is an object-oriented programming language used for enterprise applications\", \n",
    "        \"Machine learning algorithms automatically learn patterns from data\",\n",
    "        \"Deep learning is a subset of machine learning using neural networks\",\n",
    "        \"FAISS enables efficient similarity search and clustering of dense vectors\",\n",
    "        \"Elasticsearch is a distributed search and analytics engine\",\n",
    "        \"BM25 is a probabilistic ranking function used in information retrieval\",\n",
    "        \"TF-IDF measures term importance in document collections\",\n",
    "        \"Vector databases are optimized for storing high-dimensional embeddings\",\n",
    "        \"SQL databases use structured query language for data management\",\n",
    "        \"Natural language processing enables computers to understand human language\",\n",
    "        \"Computer vision algorithms process and analyze visual data\",\n",
    "        \"Recommendation systems suggest relevant items to users\",\n",
    "        \"Time series analysis forecasts future values from historical data\",\n",
    "        \"Graph databases model relationships between entities\"\n",
    "    ]\n",
    "    \n",
    "    # Add document indices to metadata\n",
    "    docs_with_metadata = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        docs_with_metadata.append(Document(page_content=doc, metadata={\"index\": i}))\n",
    "    \n",
    "    queries = [\n",
    "        \"How to program in Python?\",\n",
    "        \"What are machine learning techniques?\", \n",
    "        \"Vector similarity search methods\",\n",
    "        \"Database query languages\",\n",
    "        \"Text analysis and processing\"\n",
    "    ]\n",
    "    \n",
    "    # Ground truth: multiple relevant docs per query\n",
    "    ground_truth = [\n",
    "        [0, 1],      # Python query -> Python + Java docs\n",
    "        [2, 3],      # ML query -> ML + Deep learning\n",
    "        [4, 8],      # Vector query -> FAISS + Vector DBs  \n",
    "        [5, 9],      # Database query -> Elasticsearch + SQL\n",
    "        [7, 10, 11]  # Text analysis -> TF-IDF + NLP + CV\n",
    "    ]\n",
    "    \n",
    "    evaluator = RetrievalEvaluator()\n",
    "    \n",
    "    # Setup retrievers with metadata-enriched documents\n",
    "    retrievers = {}\n",
    "    docs = docs_with_metadata\n",
    "    \n",
    "    # FAISS\n",
    "    faiss_store = await FAISS.afrom_documents(docs, evaluator.embeddings)\n",
    "    retrievers[\"faiss\"] = faiss_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    # BM25  \n",
    "    retrievers[\"bm25\"] = BM25Retriever.from_documents(docs)\n",
    "    retrievers[\"bm25\"].k = 5\n",
    "    \n",
    "    # RRF\n",
    "    retrievers[\"rrf\"] = EnsembleRetriever(\n",
    "        retrievers=[retrievers[\"faiss\"], retrievers[\"bm25\"]],\n",
    "        weights=[0.5, 0.5]\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    results = await evaluator.evaluate_retrieval(retrievers, queries, ground_truth)\n",
    "    \n",
    "    # Print results\n",
    "    for method, metrics in results.items():\n",
    "        print(f\"\\n{method.upper()} Results:\")\n",
    "        for metric, score in metrics.items():\n",
    "            print(f\"  {metric}: {score:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\"retrieval_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vincent": {
   "sessionId": "205dfd640fc5b077051ac09e_2025-06-04T10-49-23-812Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
